version: '3.8'

services:
  core_logic_service:
    build:
      context: .
      dockerfile: core_logic_service/Dockerfile
    ports:
      - "5000:5000"
    networks:
      - inference_network

  pytorch_inference_service:
    build:
      context: .
      dockerfile: pytorch_inference_service/Dockerfile
    ports:
      - "5001:5000"
    networks:
      - inference_network
    depends_on:
      - core_logic_service

  onnx_exporter_service:
    build: ./onnx_exporter_service
    ports:
      - "5002:5002"
    networks:
      - inference_network
    depends_on:
      - core_logic_service
      - redis

  redis:
    image: "redis:alpine"
    ports:
      - "6379:6379"
    networks:
      - inference_network

networks:
  inference_network:
