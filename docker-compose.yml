version: '3.8'

services:
  core_logic_service:
    build:
      context: .
      dockerfile: core_logic_service/Dockerfile
    ports:
      - "5000:5000"
    networks:
      - inference_network

  pytorch_inference_service:
    build:
      context: .
      dockerfile: pytorch_inference_service/Dockerfile
    ports:
      - "5001:5000"
    networks:
      - inference_network
    depends_on:
      - core_logic_service

  redis:
    image: "redis:alpine"
    ports:
      - "6379:6379"
    networks:
      - inference_network

  tests:
    build:
      context: .
      dockerfile: core_logic_service/Dockerfile.tests
    networks:
      - inference_network
    depends_on:
      - core_logic_service
      - redis

networks:
  inference_network:
